{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0e063a-e009-4916-b749-6f853c644976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# # Download the WordNet resource\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"sticky rice\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)  # Output: ['coriander', 'cilantro', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968a95a4-3900-4809-ae70-38d243bdac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_linguistic_synonyms(term):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return sorted(synonyms)\n",
    "\n",
    "# Example usage\n",
    "# Limitation: WordNet often misses culinary-specific synonyms.\n",
    "print(get_linguistic_synonyms(\"sticky_rice\"))  # Note: WordNet uses underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "251a62d1-fcc7-4466-9b29-17e586c7b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7034e33c-21a3-4553-9872-0324a89042cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('glutinous rice', 100, 'synonym')]\n",
      "[('waxy rice', 62, 'synonym')]\n"
     ]
    }
   ],
   "source": [
    "# First install the required packages if you haven't\n",
    "# pip install fuzzywuzzy python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import fuzz  # Correct import\n",
    "from fuzzywuzzy import process  # For advanced processing\n",
    "\n",
    "culinary_synonyms_db = {\n",
    "    'sticky rice': ['glutinous rice', 'sweet rice', 'waxy rice', 'mochi rice'],\n",
    "    'all-purpose flour': ['plain flour', 'white flour', 'wheat flour'],\n",
    "    # Add more terms as needed\n",
    "}\n",
    "\n",
    "def find_culinary_synonyms(user_term, threshold=60):\n",
    "    matches = []\n",
    "    \n",
    "    # Search through canonical terms and their synonyms\n",
    "    for canonical, synonyms in culinary_synonyms_db.items():\n",
    "        # Compare with canonical term\n",
    "        ratio = fuzz.ratio(user_term.lower(), canonical.lower())\n",
    "        if ratio >= threshold:\n",
    "            matches.append((canonical, ratio, 'canonical'))\n",
    "        \n",
    "        # Compare with each synonym\n",
    "        for synonym in synonyms:\n",
    "            ratio = fuzz.ratio(user_term.lower(), synonym.lower())\n",
    "            if ratio >= threshold:\n",
    "                matches.append((synonym, ratio, 'synonym'))\n",
    "    \n",
    "    # Sort by highest match score\n",
    "    return sorted(matches, key=lambda x: -x[1])\n",
    "\n",
    "# Test the function\n",
    "print(find_culinary_synonyms(\"glutinous rice\"))\n",
    "print(find_culinary_synonyms(\"rice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "917e043e-0eda-4f2e-9a90-a8f1bcdb7109",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endive.n.01: widely cultivated herb with leaves valued as salad green; either curly serrated leaves or broad flat ones that are usually blanched\n",
      "Hypernyms (broader categories): ['herb.n.01']\n",
      "Hyponyms (specific types): []\n",
      "---\n",
      "chicory_escarole.n.01: variety of endive having leaves with irregular frilled edges\n",
      "Hypernyms (broader categories): ['salad_green.n.01']\n",
      "Hyponyms (specific types): ['belgian_endive.n.01']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Look at synsets for 'salmon'\n",
    "salmon = wn.synsets('endive')\n",
    "for syn in salmon:\n",
    "    print(f\"{syn.name()}: {syn.definition()}\")\n",
    "    print(\"Hypernyms (broader categories):\", [hyper.name() for hyper in syn.hypernyms()])\n",
    "    print(\"Hyponyms (specific types):\", [hypo.name() for hypo in syn.hyponyms()])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc8a589e-2be5-455f-bc2a-769c65e1b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp39-cp39-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
      "   ---------------------------------------- 0.0/662.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 662.0/662.0 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp39-cp39-win_amd64.whl (208 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.77.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d2a0ce9-bf84-425e-b978-f3fc5398554a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      5\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m         }]\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m---> 13\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_synonyms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m ans:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(syn)\n",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m, in \u001b[0;36mget_llm_synonyms\u001b[1;34m(term)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_llm_synonyms\u001b[39m(term):\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mList 5 culinary synonyms for \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mterm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m as a comma-separated list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spacy\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai  # or any other LLM provider\n",
    "\n",
    "def get_llm_synonyms(term):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"List 5 culinary synonyms for '{term}' as a comma-separated list\"\n",
    "        }]\n",
    "    )\n",
    "    return [x.strip() for x in response.choices[0].message.content.split(\",\")]\n",
    "\n",
    "ans = get_llm_synonyms('endive');\n",
    "for syn in ans:\n",
    "    print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9711e4-ad10-4a6b-a363-922f0bcc939f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import NLTK\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Download the WordNet resource\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "\n",
    "# Download the WordNet resource\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Verify it works by importing and using WordNet\n",
    "from nltk.corpus import wordnet\n",
    "print(\"WordNet successfully installed!\")\n",
    "print(f\"Example: Synsets for 'python': {wordnet.synsets('python')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6829031-8020-46f9-8090-2f950ac78c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer Results:\n",
      "----------------------------------------\n",
      "coleslaw mix              => coleslaw mix\n",
      "banana leaves             => banana leaf\n",
      "savoy cabbage leaves      => savoy cabbage leaf\n",
      "napa cabbage leaves       => napa cabbage leaf\n",
      "cauliflower florets       => cauliflower floret\n",
      "celery leaves             => celery leaf\n",
      "red belgian endive        => red belgian endive\n",
      "plain yogurt              => plain yogurt\n",
      "baby corn                 => baby corn\n",
      "low fat vanilla yogurt    => low fat vanilla yogurt\n",
      "persian cucumber          => persian cucumber\n",
      "japanese eggplant         => japanese eggplant\n",
      "bottled garlic            => bottle garlic\n",
      "bottled ginger            => bottle ginger\n",
      "lacinato kale             => lacinato kale\n",
      "mixed mushrooms           => mix mushroom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\paili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data (only needed first time)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "test_ingredients = [\n",
    "    # \"japanese eggplants\",\n",
    "    # \"persian cucumbers\",\n",
    "    # \"baby corns\",\n",
    "    # \"cauliflower florets\",\n",
    "    # \"roma tomatoes\",\n",
    "    # \"fresh basil leaves\",\n",
    "    # \"extra virgin olive oil\",\n",
    "    # \"minced garlic cloves\",\n",
    "    # \"grated parmesan cheese\",\n",
    "    # \"whole wheat spaghetti\"\n",
    "    \"coleslaw mix\",\n",
    "    \"banana leaves\",\n",
    "    \"savoy cabbage leaves\",\n",
    "    \"napa cabbage leaves\",\n",
    "    \"cauliflower florets\",\n",
    "    \"celery leaves\",\n",
    "    \"red belgian endive\",\n",
    "    \"plain yogurt\",\n",
    "    \"baby corn\",\n",
    "    \"low fat vanilla yogurt\",\n",
    "    \"persian cucumber\",\n",
    "    \"japanese eggplant\",\n",
    "    \"bottled garlic\",\n",
    "    \"bottled ginger\",\n",
    "    \"lacinato kale\",\n",
    "    \"mixed mushrooms\",\n",
    "]\n",
    "\n",
    "def normalize_with_wordnet(ingredient):\n",
    "    words = ingredient.split()\n",
    "    # Try lemmatizing as noun first, then verb if that doesn't change the word\n",
    "    lemmatized = [lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'n'), 'v') \n",
    "                 for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "print(\"WordNet Lemmatizer Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    normalized = normalize_with_wordnet(ingredient)\n",
    "    print(f\"{ingredient.ljust(25)} => {normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd35c80c-ebea-45ea-9fef-fa6c94ce5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Results:\n",
      "----------------------------------------\n",
      "Original: canned cannellini beans\n",
      "  Root: beans\n",
      "  Nouns: cannellini, bean\n",
      "  Compounds: canned, cannellini, cannellini\n",
      "--------------------\n",
      "Original: canned blackeyed peas\n",
      "  Root: canned\n",
      "  Nouns: pea\n",
      "  Compounds: blackeyed, pea\n",
      "--------------------\n",
      "Original: dried red lentils\n",
      "  Root: lentils\n",
      "  Nouns: lentil\n",
      "  Compounds: dried, red, lentil\n",
      "--------------------\n",
      "Original: canned butter beans\n",
      "  Root: beans\n",
      "  Nouns: butter, bean\n",
      "  Compounds: canned, butter, butter\n",
      "--------------------\n",
      "Original: red miso\n",
      "  Root: miso\n",
      "  Nouns: miso\n",
      "  Compounds: red, miso\n",
      "--------------------\n",
      "Original: kecap manis\n",
      "  Root: manis\n",
      "  Nouns: None\n",
      "  Compounds: kecap\n",
      "--------------------\n",
      "Original: firm tofu\n",
      "  Root: tofu\n",
      "  Nouns: firm, tofu\n",
      "  Compounds: firm, firm\n",
      "--------------------\n",
      "Original: milk substitute\n",
      "  Root: substitute\n",
      "  Nouns: milk, substitute\n",
      "  Compounds: milk, milk\n",
      "--------------------\n",
      "Original: lower sodium shoyu\n",
      "  Root: shoyu\n",
      "  Nouns: sodium, shoyu\n",
      "  Compounds: lower, sodium, sodium\n",
      "--------------------\n",
      "Original: lamb\n",
      "  Root: lamb\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: corn muffin mix\n",
      "  Root: mix\n",
      "  Nouns: corn, muffin, mix\n",
      "  Compounds: corn, corn\n",
      "--------------------\n",
      "Original: crusty bread\n",
      "  Root: bread\n",
      "  Nouns: bread\n",
      "  Compounds: crusty, bread\n",
      "--------------------\n",
      "Original: crostini\n",
      "  Root: crostini\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: wheat flatbreads\n",
      "  Root: flatbreads\n",
      "  Nouns: wheat, flatbread\n",
      "  Compounds: wheat, wheat\n",
      "--------------------\n",
      "Original: bread cubes\n",
      "  Root: cubes\n",
      "  Nouns: bread, cube\n",
      "  Compounds: bread, bread\n",
      "--------------------\n",
      "Original: panko\n",
      "  Root: panko\n",
      "  Nouns: panko\n",
      "  Compounds: panko\n",
      "--------------------\n",
      "Original: matzo\n",
      "  Root: matzo\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: oyster crackers\n",
      "  Root: crackers\n",
      "  Nouns: oyster, cracker\n",
      "  Compounds: oyster, oyster\n",
      "--------------------\n",
      "Original: shortcrust pastry\n",
      "  Root: pastry\n",
      "  Nouns: pastry\n",
      "  Compounds: shortcrust, pastry\n",
      "--------------------\n",
      "Original: sesame seed burger bun\n",
      "  Root: bun\n",
      "  Nouns: seed, burger\n",
      "  Compounds: sesame, seed, seed\n",
      "--------------------\n",
      "Original: whole wheat hot dog bun\n",
      "  Root: bun\n",
      "  Nouns: wheat, dog, bun\n",
      "  Compounds: whole, wheat\n",
      "--------------------\n",
      "Original: wrap\n",
      "  Root: wrap\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: wonton wrappers\n",
      "  Root: wrappers\n",
      "  Nouns: None\n",
      "  Compounds: wonton\n",
      "--------------------\n",
      "Original: calumet baking powder\n",
      "  Root: baking\n",
      "  Nouns: powder\n",
      "  Compounds: powder\n",
      "--------------------\n",
      "Original: flatbread\n",
      "  Root: flatbread\n",
      "  Nouns: flatbread\n",
      "  Compounds: flatbread\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_ingredients = [\n",
    "    # \"japanese eggplants\",\n",
    "    # \"persian cucumbers\",\n",
    "    # \"baby corns\",\n",
    "    # \"cauliflower florets\",\n",
    "    # \"roma tomatoes\",\n",
    "    # \"fresh basil leaves\",\n",
    "    # \"extra virgin olive oil\",\n",
    "    # \"minced garlic cloves\",\n",
    "    # \"grated parmesan cheese\",\n",
    "    # \"whole wheat spaghetti\"\n",
    "\n",
    "\"canned cannellini beans\",\n",
    "\"canned blackeyed peas\",\n",
    "\"dried red lentils\",\n",
    "\"canned butter beans\",\n",
    "\"red miso\",\n",
    "\"kecap manis\",\n",
    "\"firm tofu\",\n",
    "\"milk substitute\",\n",
    "\"lower sodium shoyu\",\n",
    "\"lamb\",\n",
    "\"corn muffin mix\",\n",
    "\"crusty bread\",\n",
    "\"crostini\",\n",
    "\"wheat flatbreads\",\n",
    "\"bread cubes\",\n",
    "\"panko\",\n",
    "\"matzo\",\n",
    "\"oyster crackers\",\n",
    "\"shortcrust pastry\",\n",
    "\"sesame seed burger bun\",\n",
    "\"whole wheat hot dog bun\",\n",
    "\"wrap\",\n",
    "\"wonton wrappers\",\n",
    "\"calumet baking powder\",\n",
    "\"flatbread\"\n",
    "]\n",
    "\n",
    "def normalize_with_spacy(ingredient):\n",
    "    doc = nlp(ingredient)\n",
    "    \n",
    "    # Strategy 1: Get the root/main noun\n",
    "    root = [token.text for token in doc if token.dep_ == 'ROOT']\n",
    "    \n",
    "    # Strategy 2: Get all nouns\n",
    "    nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    \n",
    "    # Strategy 3: Get compound nouns\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod'):\n",
    "            compounds.append(token.text)\n",
    "        if token.pos_ == 'NOUN':\n",
    "            compounds.append(token.lemma_)\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'root': root[0] if root else None,\n",
    "        'nouns': ', '.join(nouns) if nouns else None,\n",
    "        'compounds': ', '.join(compounds) if compounds else None\n",
    "    }\n",
    "\n",
    "print(\"\\nspaCy Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    results = normalize_with_spacy(ingredient)\n",
    "    print(f\"Original: {ingredient}\")\n",
    "    print(f\"  Root: {results['root']}\")\n",
    "    print(f\"  Nouns: {results['nouns']}\")\n",
    "    print(f\"  Compounds: {results['compounds']}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf116a4a-2672-4d8e-8ec5-0d5458ca73cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Results:\n",
      "----------------------------------------\n",
      "Original: whole garlic cloves\n",
      "  Result: clove\n",
      "--------------------\n",
      "Original: halibut fillets\n",
      "  Result: fillet\n",
      "--------------------\n",
      "Original: sesame seeds\n",
      "  Result: seed\n",
      "--------------------\n",
      "Original: whole grain mustard\n",
      "  Result: grain mustard\n",
      "--------------------\n",
      "Original: lower sodium soy sauce\n",
      "  Result: sodium soy sauce\n",
      "--------------------\n",
      "Original: lemon wedge\n",
      "  Result: wedge\n",
      "--------------------\n",
      "Original: low sodium chicken broth\n",
      "  Result: sodium chicken broth\n",
      "--------------------\n",
      "Original: corn on the cob\n",
      "  Result: corn cob\n",
      "--------------------\n",
      "Original: garlic powder\n",
      "  Result: powder\n",
      "--------------------\n",
      "Original: ahi tuna steak\n",
      "  Result: steak\n",
      "--------------------\n",
      "Original: collard greens\n",
      "  Result: green\n",
      "--------------------\n",
      "Original: fennel seeds\n",
      "  Result: seed\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_ingredients = [\n",
    "\"whole garlic cloves\",\n",
    "\"halibut fillets\",\n",
    "\"sesame seeds\",\n",
    "\"whole grain mustard\",\n",
    "\"lower sodium soy sauce\",\n",
    "\"lemon wedge\",\n",
    "\"low sodium chicken broth\",\n",
    "\"corn on the cob\",\n",
    "\"garlic powder\",\n",
    "\"ahi tuna steak\",\n",
    "\"collard greens\",\n",
    "\"fennel seeds\"\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "def normalize_ingredient(name):\n",
    "    # Check against known exceptions first\n",
    "    # for pattern, replacement in normalization_rules.items():\n",
    "    #     if re.search(pattern, name, re.I):\n",
    "    #         return replacement\n",
    "    \n",
    "    # Otherwise use NLP approach\n",
    "    doc = nlp(name.lower())\n",
    "    # Get nouns and compound nouns\n",
    "    # tokens = [token.lemma_ for token in doc if token.pos_ in ('NOUN', 'PROPN')]\n",
    "    tokens = [token.lemma_ for token in doc if token.pos_ in ('NOUN')]\n",
    "\n",
    "  \n",
    "    # return ' '.join(tokens[-1:])  # Take the last noun as most important\n",
    "    return ' '.join(tokens)  # Take the last noun as most important\n",
    "\n",
    "    # nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    # return max(set(nouns), key=nouns.count) if nouns else name\n",
    "\n",
    "\n",
    "print(\"\\nspaCy Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    result = normalize_ingredient(ingredient)\n",
    "    print(f\"Original: {ingredient}\")\n",
    "    print(f\"  Result: {result}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b7c8fa-9311-47dd-b56a-d845a0a23b6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy language model...\n",
      "Found 2304 new ingredients to process...\n",
      "Processed 100/2304\n",
      "Processed 200/2304\n",
      "Processed 300/2304\n",
      "Processed 400/2304\n",
      "Processed 500/2304\n",
      "Processed 600/2304\n",
      "Processed 700/2304\n",
      "Processed 800/2304\n",
      "Processed 900/2304\n",
      "Processed 1000/2304\n",
      "Processed 1100/2304\n",
      "Processed 1200/2304\n",
      "Processed 1300/2304\n",
      "Processed 1400/2304\n",
      "Processed 1500/2304\n",
      "Processed 1600/2304\n",
      "Processed 1700/2304\n",
      "Processed 1800/2304\n",
      "Processed 1900/2304\n",
      "Processed 2000/2304\n",
      "Processed 2100/2304\n",
      "Processed 2200/2304\n",
      "Processed 2300/2304\n",
      "Processed 2304/2304\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Initialize NLP processor\n",
    "print(\"Loading spaCy language model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# MSSQL Database configuration with Integrated Security\n",
    "DB_CONFIG = {\n",
    "    'driver': 'ODBC Driver 17 for SQL Server',\n",
    "    'server': '(localdb)\\\\MSSQLLocalDB',\n",
    "    'database': 'GroceryDB',\n",
    "    'schema': 'dbo',\n",
    "    'ingredients_table': 'Ingredients',\n",
    "    'results_table': 'IngredientName'\n",
    "}\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Create SQLAlchemy engine for MSSQL using Windows Authentication\"\"\"\n",
    "    connection_string = (\n",
    "        f\"mssql+pyodbc://{DB_CONFIG['server']}/{DB_CONFIG['database']}?\"\n",
    "        f\"driver={DB_CONFIG['driver']}&\"\n",
    "        f\"trusted_connection=yes\"\n",
    "    )\n",
    "    # connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=(localdb)\\\\MSSQLLocalDB;DATABASE=GroceryDB;Trusted_Connection=yes;\"\n",
    "    return create_engine(connection_string)\n",
    "\n",
    "def extract_nouns(text):\n",
    "    \"\"\"Extract nouns from text using spaCy\"\"\"\n",
    "    if not text or str(text).strip() == '':\n",
    "        return None, None\n",
    "        \n",
    "    doc = nlp(str(text).lower())\n",
    "    nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    \n",
    "    last_noun = nouns[-1] if nouns else text\n",
    "    all_nouns = ' '.join(nouns) if nouns else text\n",
    "    \n",
    "    return last_noun, all_nouns\n",
    "\n",
    "def get_unprocessed_ingredients(engine):\n",
    "    \"\"\"Retrieve only ingredients that haven't been processed yet\"\"\"\n",
    "    metadata = MetaData()\n",
    "    \n",
    "    ingredients = Table(\n",
    "        DB_CONFIG['ingredients_table'],\n",
    "        metadata,\n",
    "        autoload_with=engine,\n",
    "        schema=DB_CONFIG['schema']\n",
    "    )\n",
    "    \n",
    "    processed = Table(\n",
    "        DB_CONFIG['results_table'],\n",
    "        metadata,\n",
    "        autoload_with=engine,\n",
    "        schema=DB_CONFIG['schema']\n",
    "    )\n",
    "    \n",
    "    query = select(\n",
    "        ingredients.c.IngredientId,\n",
    "        ingredients.c.Name\n",
    "    ).select_from(\n",
    "        ingredients.outerjoin(\n",
    "            processed,\n",
    "            ingredients.c.IngredientId == processed.c.IngredientId\n",
    "        )\n",
    "    ).where(\n",
    "        processed.c.IngredientId == None\n",
    "    )\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)\n",
    "        return result.fetchall()\n",
    "\n",
    "def process_ingredients():\n",
    "    \"\"\"Main processing function with incremental update support\"\"\"\n",
    "    engine = get_db_engine()\n",
    "    \n",
    "    try:\n",
    "        # Get only unprocessed ingredients\n",
    "        unprocessed = get_unprocessed_ingredients(engine)\n",
    "        \n",
    "        if not unprocessed:\n",
    "            print(\"No new ingredients to process\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(unprocessed)} new ingredients to process...\")\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(unprocessed), batch_size):\n",
    "            batch = unprocessed[i:i + batch_size]\n",
    "            processed_data = []\n",
    "            \n",
    "            for id, name in batch:\n",
    "                try:\n",
    "                    last_noun, all_nouns = extract_nouns(name)\n",
    "                    processed_data.append({\n",
    "                        'IngredientId': id,\n",
    "                        'OriginalName': name,\n",
    "                        'LastNoun': last_noun,\n",
    "                        'AllNouns': all_nouns\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    processed_data.append({\n",
    "                        'IngredientId': id,\n",
    "                        'OriginalName': name,\n",
    "                        'LastNoun': name,\n",
    "                        'AllNouns': name\n",
    "                    })\n",
    "            \n",
    "            # Insert batch into database\n",
    "            if processed_data:\n",
    "                insert_stmt = f\"\"\"\n",
    "                INSERT INTO {DB_CONFIG['schema']}.{DB_CONFIG['results_table']} \n",
    "                (IngredientId, OriginalName, LastNoun, Processed)\n",
    "                VALUES (:IngredientId, :OriginalName, :LastNoun, :AllNouns)\n",
    "                \"\"\"\n",
    "                with engine.begin() as conn:\n",
    "                    conn.execute(text(insert_stmt), processed_data)\n",
    "            \n",
    "            print(f\"Processed {min(i + batch_size, len(unprocessed))}/{len(unprocessed)}\")\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368d515c-de41-499e-afec-dfcb99765d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.1-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.1-cp39-cp39-win_amd64.whl (294 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.1 sqlalchemy-2.0.40\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2933d79b-8007-499e-8a4e-1f46c77bc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyodbc\n",
      "  Downloading pyodbc-5.2.0-cp39-cp39-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading pyodbc-5.2.0-cp39-cp39-win_amd64.whl (68 kB)\n",
      "Installing collected packages: pyodbc\n",
      "Successfully installed pyodbc-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6da79d9-11e2-4d31-99ef-6e030eb2e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2844200884719612e-05\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "def get_term_frequency(term):\n",
    "    # Use Google Ngrams or other corpus data\n",
    "    response = requests.get(f\"https://books.google.com/ngrams/json?content={term}&year_start=2018\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data[0]['timeseries'][-1] if data else 0\n",
    "    return 0\n",
    "\n",
    "print(get_term_frequency(\"rice\"))  # Returns usage frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13369281-35d5-4ba7-9332-70cc1f44c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated 1951 records.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pyodbc  # or another database connector like pymssql, sqlalchemy\n",
    "\n",
    "DB_CONNECTION_STRING = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=(localdb)\\\\MSSQLLocalDB;DATABASE=GroceryDB;Trusted_Connection=yes;\"\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = \"C:\\\\Users\\\\paili\\\\OneDrive\\\\Documents\\\\SmartGroceryAssistant\\\\IngredientNameReview.csv\"\n",
    "\n",
    "# Establish database connection\n",
    "try:\n",
    "    conn = pyodbc.connect(DB_CONNECTION_STRING)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Read CSV file and update records\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            ingredient_id = row['ingredientId']\n",
    "            curated_value = row['Curated']\n",
    "            \n",
    "            # Update query\n",
    "            update_query = \"\"\"\n",
    "            UPDATE GroceryDB.dbo.IngredientName\n",
    "            SET Curated = ?\n",
    "            WHERE ingredientId = ?\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute update\n",
    "            cursor.execute(update_query, (curated_value, ingredient_id))\n",
    "        \n",
    "    # Commit all changes\n",
    "    conn.commit()\n",
    "    print(f\"Successfully updated {reader.line_num - 1} records.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    if 'conn' in locals():\n",
    "        conn.rollback()\n",
    "        \n",
    "finally:\n",
    "    if 'conn' in locals():\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab94282-bc49-4a0e-80e9-20664092f135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spacy)",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
