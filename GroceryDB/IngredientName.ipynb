{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab0e063a-e009-4916-b749-6f853c644976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eggplant', 'aubergine', 'mad_apple', 'eggplant', 'aubergine', 'brinjal', 'eggplant_bush', 'garden_egg', 'mad_apple', 'Solanum_melongena']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"eggplant\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)  # Output: ['coriander', 'cilantro', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9711e4-ad10-4a6b-a363-922f0bcc939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paili\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet successfully installed!\n",
      "Example: Synsets for 'python': [Synset('python.n.01'), Synset('python.n.02'), Synset('python.n.03')]\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "\n",
    "# Download the WordNet resource\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Verify it works by importing and using WordNet\n",
    "from nltk.corpus import wordnet\n",
    "print(\"WordNet successfully installed!\")\n",
    "print(f\"Example: Synsets for 'python': {wordnet.synsets('python')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6829031-8020-46f9-8090-2f950ac78c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer Results:\n",
      "----------------------------------------\n",
      "coleslaw mix              => coleslaw mix\n",
      "banana leaves             => banana leaf\n",
      "savoy cabbage leaves      => savoy cabbage leaf\n",
      "napa cabbage leaves       => napa cabbage leaf\n",
      "cauliflower florets       => cauliflower floret\n",
      "celery leaves             => celery leaf\n",
      "red belgian endive        => red belgian endive\n",
      "plain yogurt              => plain yogurt\n",
      "baby corn                 => baby corn\n",
      "low fat vanilla yogurt    => low fat vanilla yogurt\n",
      "persian cucumber          => persian cucumber\n",
      "japanese eggplant         => japanese eggplant\n",
      "bottled garlic            => bottle garlic\n",
      "bottled ginger            => bottle ginger\n",
      "lacinato kale             => lacinato kale\n",
      "mixed mushrooms           => mix mushroom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\paili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data (only needed first time)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "test_ingredients = [\n",
    "    # \"japanese eggplants\",\n",
    "    # \"persian cucumbers\",\n",
    "    # \"baby corns\",\n",
    "    # \"cauliflower florets\",\n",
    "    # \"roma tomatoes\",\n",
    "    # \"fresh basil leaves\",\n",
    "    # \"extra virgin olive oil\",\n",
    "    # \"minced garlic cloves\",\n",
    "    # \"grated parmesan cheese\",\n",
    "    # \"whole wheat spaghetti\"\n",
    "    \"coleslaw mix\",\n",
    "    \"banana leaves\",\n",
    "    \"savoy cabbage leaves\",\n",
    "    \"napa cabbage leaves\",\n",
    "    \"cauliflower florets\",\n",
    "    \"celery leaves\",\n",
    "    \"red belgian endive\",\n",
    "    \"plain yogurt\",\n",
    "    \"baby corn\",\n",
    "    \"low fat vanilla yogurt\",\n",
    "    \"persian cucumber\",\n",
    "    \"japanese eggplant\",\n",
    "    \"bottled garlic\",\n",
    "    \"bottled ginger\",\n",
    "    \"lacinato kale\",\n",
    "    \"mixed mushrooms\",\n",
    "]\n",
    "\n",
    "def normalize_with_wordnet(ingredient):\n",
    "    words = ingredient.split()\n",
    "    # Try lemmatizing as noun first, then verb if that doesn't change the word\n",
    "    lemmatized = [lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'n'), 'v') \n",
    "                 for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "print(\"WordNet Lemmatizer Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    normalized = normalize_with_wordnet(ingredient)\n",
    "    print(f\"{ingredient.ljust(25)} => {normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd35c80c-ebea-45ea-9fef-fa6c94ce5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Results:\n",
      "----------------------------------------\n",
      "Original: canned cannellini beans\n",
      "  Root: beans\n",
      "  Nouns: cannellini, bean\n",
      "  Compounds: canned, cannellini, cannellini\n",
      "--------------------\n",
      "Original: canned blackeyed peas\n",
      "  Root: canned\n",
      "  Nouns: pea\n",
      "  Compounds: blackeyed, pea\n",
      "--------------------\n",
      "Original: dried red lentils\n",
      "  Root: lentils\n",
      "  Nouns: lentil\n",
      "  Compounds: dried, red, lentil\n",
      "--------------------\n",
      "Original: canned butter beans\n",
      "  Root: beans\n",
      "  Nouns: butter, bean\n",
      "  Compounds: canned, butter, butter\n",
      "--------------------\n",
      "Original: red miso\n",
      "  Root: miso\n",
      "  Nouns: miso\n",
      "  Compounds: red, miso\n",
      "--------------------\n",
      "Original: kecap manis\n",
      "  Root: manis\n",
      "  Nouns: None\n",
      "  Compounds: kecap\n",
      "--------------------\n",
      "Original: firm tofu\n",
      "  Root: tofu\n",
      "  Nouns: firm, tofu\n",
      "  Compounds: firm, firm\n",
      "--------------------\n",
      "Original: milk substitute\n",
      "  Root: substitute\n",
      "  Nouns: milk, substitute\n",
      "  Compounds: milk, milk\n",
      "--------------------\n",
      "Original: lower sodium shoyu\n",
      "  Root: shoyu\n",
      "  Nouns: sodium, shoyu\n",
      "  Compounds: lower, sodium, sodium\n",
      "--------------------\n",
      "Original: lamb\n",
      "  Root: lamb\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: corn muffin mix\n",
      "  Root: mix\n",
      "  Nouns: corn, muffin, mix\n",
      "  Compounds: corn, corn\n",
      "--------------------\n",
      "Original: crusty bread\n",
      "  Root: bread\n",
      "  Nouns: bread\n",
      "  Compounds: crusty, bread\n",
      "--------------------\n",
      "Original: crostini\n",
      "  Root: crostini\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: wheat flatbreads\n",
      "  Root: flatbreads\n",
      "  Nouns: wheat, flatbread\n",
      "  Compounds: wheat, wheat\n",
      "--------------------\n",
      "Original: bread cubes\n",
      "  Root: cubes\n",
      "  Nouns: bread, cube\n",
      "  Compounds: bread, bread\n",
      "--------------------\n",
      "Original: panko\n",
      "  Root: panko\n",
      "  Nouns: panko\n",
      "  Compounds: panko\n",
      "--------------------\n",
      "Original: matzo\n",
      "  Root: matzo\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: oyster crackers\n",
      "  Root: crackers\n",
      "  Nouns: oyster, cracker\n",
      "  Compounds: oyster, oyster\n",
      "--------------------\n",
      "Original: shortcrust pastry\n",
      "  Root: pastry\n",
      "  Nouns: pastry\n",
      "  Compounds: shortcrust, pastry\n",
      "--------------------\n",
      "Original: sesame seed burger bun\n",
      "  Root: bun\n",
      "  Nouns: seed, burger\n",
      "  Compounds: sesame, seed, seed\n",
      "--------------------\n",
      "Original: whole wheat hot dog bun\n",
      "  Root: bun\n",
      "  Nouns: wheat, dog, bun\n",
      "  Compounds: whole, wheat\n",
      "--------------------\n",
      "Original: wrap\n",
      "  Root: wrap\n",
      "  Nouns: None\n",
      "  Compounds: None\n",
      "--------------------\n",
      "Original: wonton wrappers\n",
      "  Root: wrappers\n",
      "  Nouns: None\n",
      "  Compounds: wonton\n",
      "--------------------\n",
      "Original: calumet baking powder\n",
      "  Root: baking\n",
      "  Nouns: powder\n",
      "  Compounds: powder\n",
      "--------------------\n",
      "Original: flatbread\n",
      "  Root: flatbread\n",
      "  Nouns: flatbread\n",
      "  Compounds: flatbread\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_ingredients = [\n",
    "    # \"japanese eggplants\",\n",
    "    # \"persian cucumbers\",\n",
    "    # \"baby corns\",\n",
    "    # \"cauliflower florets\",\n",
    "    # \"roma tomatoes\",\n",
    "    # \"fresh basil leaves\",\n",
    "    # \"extra virgin olive oil\",\n",
    "    # \"minced garlic cloves\",\n",
    "    # \"grated parmesan cheese\",\n",
    "    # \"whole wheat spaghetti\"\n",
    "\n",
    "\"canned cannellini beans\",\n",
    "\"canned blackeyed peas\",\n",
    "\"dried red lentils\",\n",
    "\"canned butter beans\",\n",
    "\"red miso\",\n",
    "\"kecap manis\",\n",
    "\"firm tofu\",\n",
    "\"milk substitute\",\n",
    "\"lower sodium shoyu\",\n",
    "\"lamb\",\n",
    "\"corn muffin mix\",\n",
    "\"crusty bread\",\n",
    "\"crostini\",\n",
    "\"wheat flatbreads\",\n",
    "\"bread cubes\",\n",
    "\"panko\",\n",
    "\"matzo\",\n",
    "\"oyster crackers\",\n",
    "\"shortcrust pastry\",\n",
    "\"sesame seed burger bun\",\n",
    "\"whole wheat hot dog bun\",\n",
    "\"wrap\",\n",
    "\"wonton wrappers\",\n",
    "\"calumet baking powder\",\n",
    "\"flatbread\"\n",
    "]\n",
    "\n",
    "def normalize_with_spacy(ingredient):\n",
    "    doc = nlp(ingredient)\n",
    "    \n",
    "    # Strategy 1: Get the root/main noun\n",
    "    root = [token.text for token in doc if token.dep_ == 'ROOT']\n",
    "    \n",
    "    # Strategy 2: Get all nouns\n",
    "    nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    \n",
    "    # Strategy 3: Get compound nouns\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod'):\n",
    "            compounds.append(token.text)\n",
    "        if token.pos_ == 'NOUN':\n",
    "            compounds.append(token.lemma_)\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'root': root[0] if root else None,\n",
    "        'nouns': ', '.join(nouns) if nouns else None,\n",
    "        'compounds': ', '.join(compounds) if compounds else None\n",
    "    }\n",
    "\n",
    "print(\"\\nspaCy Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    results = normalize_with_spacy(ingredient)\n",
    "    print(f\"Original: {ingredient}\")\n",
    "    print(f\"  Root: {results['root']}\")\n",
    "    print(f\"  Nouns: {results['nouns']}\")\n",
    "    print(f\"  Compounds: {results['compounds']}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf116a4a-2672-4d8e-8ec5-0d5458ca73cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Results:\n",
      "----------------------------------------\n",
      "Original: canned cannellini beans\n",
      "  Result: bean\n",
      "--------------------\n",
      "Original: canned blackeyed peas\n",
      "  Result: pea\n",
      "--------------------\n",
      "Original: dried red lentils\n",
      "  Result: lentil\n",
      "--------------------\n",
      "Original: canned butter beans\n",
      "  Result: bean\n",
      "--------------------\n",
      "Original: red miso\n",
      "  Result: miso\n",
      "--------------------\n",
      "Original: kecap manis\n",
      "  Result: kecap manis\n",
      "--------------------\n",
      "Original: firm tofu\n",
      "  Result: firm\n",
      "--------------------\n",
      "Original: milk substitute\n",
      "  Result: milk\n",
      "--------------------\n",
      "Original: lower sodium shoyu\n",
      "  Result: shoyu\n",
      "--------------------\n",
      "Original: lamb\n",
      "  Result: lamb\n",
      "--------------------\n",
      "Original: corn muffin mix\n",
      "  Result: corn\n",
      "--------------------\n",
      "Original: crusty bread\n",
      "  Result: bread\n",
      "--------------------\n",
      "Original: crostini\n",
      "  Result: crostini\n",
      "--------------------\n",
      "Original: wheat flatbreads\n",
      "  Result: wheat\n",
      "--------------------\n",
      "Original: bread cubes\n",
      "  Result: cube\n",
      "--------------------\n",
      "Original: panko\n",
      "  Result: panko\n",
      "--------------------\n",
      "Original: matzo\n",
      "  Result: matzo\n",
      "--------------------\n",
      "Original: oyster crackers\n",
      "  Result: cracker\n",
      "--------------------\n",
      "Original: shortcrust pastry\n",
      "  Result: pastry\n",
      "--------------------\n",
      "Original: sesame seed burger bun\n",
      "  Result: seed\n",
      "--------------------\n",
      "Original: whole wheat hot dog bun\n",
      "  Result: wheat\n",
      "--------------------\n",
      "Original: wrap\n",
      "  Result: wrap\n",
      "--------------------\n",
      "Original: wonton wrappers\n",
      "  Result: wonton wrappers\n",
      "--------------------\n",
      "Original: calumet baking powder\n",
      "  Result: powder\n",
      "--------------------\n",
      "Original: flatbread\n",
      "  Result: flatbread\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_ingredients = [\n",
    "    # \"japanese eggplants\",\n",
    "    # \"persian cucumbers\",\n",
    "    # \"baby corns\",\n",
    "    # \"cauliflower florets\",\n",
    "    # \"roma tomatoes\",\n",
    "    # \"fresh basil leaves\",\n",
    "    # \"extra virgin olive oil\",\n",
    "    # \"minced garlic cloves\",\n",
    "    # \"grated parmesan cheese\",\n",
    "    # \"whole wheat spaghetti\"\n",
    "\n",
    "\"canned cannellini beans\",\n",
    "\"canned blackeyed peas\",\n",
    "\"dried red lentils\",\n",
    "\"canned butter beans\",\n",
    "\"red miso\",\n",
    "\"kecap manis\",\n",
    "\"firm tofu\",\n",
    "\"milk substitute\",\n",
    "\"lower sodium shoyu\",\n",
    "\"lamb\",\n",
    "\"corn muffin mix\",\n",
    "\"crusty bread\",\n",
    "\"crostini\",\n",
    "\"wheat flatbreads\",\n",
    "\"bread cubes\",\n",
    "\"panko\",\n",
    "\"matzo\",\n",
    "\"oyster crackers\",\n",
    "\"shortcrust pastry\",\n",
    "\"sesame seed burger bun\",\n",
    "\"whole wheat hot dog bun\",\n",
    "\"wrap\",\n",
    "\"wonton wrappers\",\n",
    "\"calumet baking powder\",\n",
    "\"flatbread\"\n",
    "]\n",
    "\n",
    "def normalize_ingredient(name):\n",
    "    # Check against known exceptions first\n",
    "    # for pattern, replacement in normalization_rules.items():\n",
    "    #     if re.search(pattern, name, re.I):\n",
    "    #         return replacement\n",
    "    \n",
    "    # Otherwise use NLP approach\n",
    "    doc = nlp(name.lower())\n",
    "    # Get nouns and compound nouns\n",
    "    tokens = [token.lemma_ for token in doc if token.pos_ in ('NOUN', 'PROPN')]\n",
    "    return ' '.join(tokens[-1:])  # Take the last noun as most important\n",
    "\n",
    "    # nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    # return max(set(nouns), key=nouns.count) if nouns else name\n",
    "\n",
    "\n",
    "print(\"\\nspaCy Results:\")\n",
    "print(\"-\" * 40)\n",
    "for ingredient in test_ingredients:\n",
    "    result = normalize_ingredient(ingredient)\n",
    "    print(f\"Original: {ingredient}\")\n",
    "    print(f\"  Result: {result}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b7c8fa-9311-47dd-b56a-d845a0a23b6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy language model...\n",
      "Found 1828 new ingredients to process...\n",
      "Processed 100/1828\n",
      "Processed 200/1828\n",
      "Processed 300/1828\n",
      "Processed 400/1828\n",
      "Processed 500/1828\n",
      "Processed 600/1828\n",
      "Processed 700/1828\n",
      "Processed 800/1828\n",
      "Processed 900/1828\n",
      "Processed 1000/1828\n",
      "Processed 1100/1828\n",
      "Processed 1200/1828\n",
      "Processed 1300/1828\n",
      "Processed 1400/1828\n",
      "Processed 1500/1828\n",
      "Processed 1600/1828\n",
      "Processed 1700/1828\n",
      "Processed 1800/1828\n",
      "Processed 1828/1828\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Initialize NLP processor\n",
    "print(\"Loading spaCy language model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# MSSQL Database configuration with Integrated Security\n",
    "DB_CONFIG = {\n",
    "    'driver': 'ODBC Driver 17 for SQL Server',\n",
    "    'server': '(localdb)\\\\MSSQLLocalDB',\n",
    "    'database': 'GroceryDB',\n",
    "    'schema': 'dbo',\n",
    "    'ingredients_table': 'Ingredients',\n",
    "    'results_table': 'IngredientName'\n",
    "}\n",
    "\n",
    "def get_db_engine():\n",
    "    \"\"\"Create SQLAlchemy engine for MSSQL using Windows Authentication\"\"\"\n",
    "    connection_string = (\n",
    "        f\"mssql+pyodbc://{DB_CONFIG['server']}/{DB_CONFIG['database']}?\"\n",
    "        f\"driver={DB_CONFIG['driver']}&\"\n",
    "        f\"trusted_connection=yes\"\n",
    "    )\n",
    "    # connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=(localdb)\\\\MSSQLLocalDB;DATABASE=GroceryDB;Trusted_Connection=yes;\"\n",
    "    return create_engine(connection_string)\n",
    "\n",
    "def extract_nouns(text):\n",
    "    \"\"\"Extract nouns from text using spaCy\"\"\"\n",
    "    if not text or str(text).strip() == '':\n",
    "        return None, None\n",
    "        \n",
    "    doc = nlp(str(text).lower())\n",
    "    nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "    \n",
    "    last_noun = nouns[-1] if nouns else text\n",
    "    all_nouns = ' '.join(nouns) if nouns else text\n",
    "    \n",
    "    return last_noun, all_nouns\n",
    "\n",
    "def get_unprocessed_ingredients(engine):\n",
    "    \"\"\"Retrieve only ingredients that haven't been processed yet\"\"\"\n",
    "    metadata = MetaData()\n",
    "    \n",
    "    ingredients = Table(\n",
    "        DB_CONFIG['ingredients_table'],\n",
    "        metadata,\n",
    "        autoload_with=engine,\n",
    "        schema=DB_CONFIG['schema']\n",
    "    )\n",
    "    \n",
    "    processed = Table(\n",
    "        DB_CONFIG['results_table'],\n",
    "        metadata,\n",
    "        autoload_with=engine,\n",
    "        schema=DB_CONFIG['schema']\n",
    "    )\n",
    "    \n",
    "    query = select(\n",
    "        ingredients.c.IngredientId,\n",
    "        ingredients.c.Name\n",
    "    ).select_from(\n",
    "        ingredients.outerjoin(\n",
    "            processed,\n",
    "            ingredients.c.IngredientId == processed.c.IngredientId\n",
    "        )\n",
    "    ).where(\n",
    "        processed.c.IngredientId == None\n",
    "    )\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)\n",
    "        return result.fetchall()\n",
    "\n",
    "def process_ingredients():\n",
    "    \"\"\"Main processing function with incremental update support\"\"\"\n",
    "    engine = get_db_engine()\n",
    "    \n",
    "    try:\n",
    "        # Get only unprocessed ingredients\n",
    "        unprocessed = get_unprocessed_ingredients(engine)\n",
    "        \n",
    "        if not unprocessed:\n",
    "            print(\"No new ingredients to process\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(unprocessed)} new ingredients to process...\")\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(unprocessed), batch_size):\n",
    "            batch = unprocessed[i:i + batch_size]\n",
    "            processed_data = []\n",
    "            \n",
    "            for id, name in batch:\n",
    "                try:\n",
    "                    last_noun, all_nouns = extract_nouns(name)\n",
    "                    processed_data.append({\n",
    "                        'IngredientId': id,\n",
    "                        'OriginalName': name,\n",
    "                        'LastNoun': last_noun,\n",
    "                        'AllNouns': all_nouns\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    processed_data.append({\n",
    "                        'IngredientId': id,\n",
    "                        'OriginalName': name,\n",
    "                        'LastNoun': name,\n",
    "                        'AllNouns': name\n",
    "                    })\n",
    "            \n",
    "            # Insert batch into database\n",
    "            if processed_data:\n",
    "                insert_stmt = f\"\"\"\n",
    "                INSERT INTO {DB_CONFIG['schema']}.{DB_CONFIG['results_table']} \n",
    "                (IngredientId, OriginalName, LastNoun, Processed)\n",
    "                VALUES (:IngredientId, :OriginalName, :LastNoun, :AllNouns)\n",
    "                \"\"\"\n",
    "                with engine.begin() as conn:\n",
    "                    conn.execute(text(insert_stmt), processed_data)\n",
    "            \n",
    "            print(f\"Processed {min(i + batch_size, len(unprocessed))}/{len(unprocessed)}\")\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368d515c-de41-499e-afec-dfcb99765d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.1-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\paili\\anaconda3\\envs\\spacy\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading sqlalchemy-2.0.40-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.1-cp39-cp39-win_amd64.whl (294 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.1 sqlalchemy-2.0.40\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2933d79b-8007-499e-8a4e-1f46c77bc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyodbc\n",
      "  Downloading pyodbc-5.2.0-cp39-cp39-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading pyodbc-5.2.0-cp39-cp39-win_amd64.whl (68 kB)\n",
      "Installing collected packages: pyodbc\n",
      "Successfully installed pyodbc-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da79d9-11e2-4d31-99ef-6e030eb2e146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spacy)",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
