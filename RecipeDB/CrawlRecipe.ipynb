{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ef6c84-ef85-4500-a59c-1b1c24f749a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-starting from recipe ID 18532\n",
      "Processing IDs 18532 to 18541 (0/10 processed)\n",
      "Successfully downloaded image for recipe 18532 as 18532.jpg\n",
      "Successfully downloaded image for recipe 18533 as 18533.jpg\n",
      "Successfully downloaded image for recipe 18534 as 18534.jpg\n",
      "Failed to download image for recipe 18535\n",
      "Successfully downloaded image for recipe 18536 as 18536.jpg\n",
      "Successfully downloaded image for recipe 18537 as 18537.jpg\n",
      "Successfully downloaded image for recipe 18538 as 18538.jpg\n",
      "Successfully downloaded image for recipe 18539 as 18539.jpg\n",
      "Successfully downloaded image for recipe 18540 as 18540.jpg\n",
      "Successfully downloaded image for recipe 18541 as 18541.jpg\n",
      "Completed crawling 10 recipes (target: 10)\n",
      "Last successful ID: 18541\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pyodbc\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class SpoonacularCrawler:\n",
    "    def __init__(self, api_key, db_connection_string, image_storage_path, \n",
    "                 max_workers=1, request_timeout=30):\n",
    "        self.api_key = api_key\n",
    "        self.db_connection_string = db_connection_string\n",
    "        self.image_storage_path = image_storage_path\n",
    "        self.max_workers = max_workers\n",
    "        self.request_timeout = request_timeout\n",
    "        self.base_url = \"https://api.spoonacular.com/recipes/{recipe_id}/information?includeNutrition=false&apiKey={api_key}\"\n",
    "        \n",
    "        # Ensure image storage directory exists\n",
    "        os.makedirs(self.image_storage_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize database connection\n",
    "        self.conn = pyodbc.connect(db_connection_string)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.processed_ids = self._load_processed_ids()\n",
    "\n",
    "    def _load_processed_ids(self):\n",
    "        \"\"\"Load all already processed recipe IDs from database\"\"\"\n",
    "        self.cursor.execute(\"SELECT recipeId FROM RawRecipeData\")\n",
    "        return {row[0] for row in self.cursor.fetchall()}\n",
    "\n",
    "    def download_image(self, recipe_id, image_url):\n",
    "        if not image_url:\n",
    "            return False, None\n",
    "        \n",
    "        try:\n",
    "            # Get the image file extension from URL\n",
    "            parsed = urlparse(image_url)\n",
    "            file_ext = os.path.splitext(parsed.path)[1].lower()\n",
    "            if not file_ext:\n",
    "                return False, None\n",
    "                \n",
    "            # Remove leading dot if present\n",
    "            file_ext = file_ext[1:] if file_ext.startswith('.') else file_ext\n",
    "            \n",
    "            # Create filename\n",
    "            filename = f\"{recipe_id}.{file_ext}\"\n",
    "            filepath = os.path.join(self.image_storage_path, filename)\n",
    "            \n",
    "            # Download the image\n",
    "            response = requests.get(image_url, stream=True, timeout=self.request_timeout)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Update database with download status\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    UPDATE Recipes \n",
    "                    SET imageDownloaded = 1, imageFileType = ?\n",
    "                    WHERE id = ?\n",
    "                \"\"\", file_ext, recipe_id)\n",
    "                self.conn.commit()\n",
    "                \n",
    "                return True, file_ext\n",
    "            return False, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image for recipe {recipe_id}: {str(e)}\")\n",
    "            return False, None\n",
    "   \n",
    "    \n",
    "    def fetch_recipe(self, recipe_id):\n",
    "        url = self.base_url.format(recipe_id=recipe_id, api_key=self.api_key)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.request_timeout)\n",
    "            if response.status_code == 200:\n",
    "                return recipe_id, response.json()\n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Recipe {recipe_id} not found\")\n",
    "                return recipe_id, None\n",
    "            else:\n",
    "                print(f\"Error fetching recipe {recipe_id}: HTTP {response.status_code}\")\n",
    "                return recipe_id, None\n",
    "        except requests.Timeout:\n",
    "            print(f\"Timeout fetching recipe {recipe_id}\")\n",
    "            return recipe_id, None\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching recipe {recipe_id}: {str(e)}\")\n",
    "            return recipe_id, None\n",
    "    \n",
    "    def save_raw_response(self, recipe_id, response):\n",
    "        if response is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO RawRecipeData (recipeId, rawResponse, fetchDateTime)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", recipe_id, json.dumps(response), datetime.now(timezone.utc))\n",
    "            self.conn.commit()\n",
    "            return True\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error saving raw response for recipe {recipe_id}: {str(e)}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "    \n",
    "    def parse_and_save_recipe(self, recipe_id, response):\n",
    "        if response is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Extract all recipe data including new fields\n",
    "            recipe_data = {\n",
    "                'id': response.get('id'),\n",
    "                'image': response.get('image'),\n",
    "                'title': response.get('title'),\n",
    "                'readyInMinutes': response.get('readyInMinutes'),\n",
    "                'servings': response.get('servings'),\n",
    "                'sourceUrl': response.get('sourceUrl'),\n",
    "                'sourceName': response.get('sourceName'),\n",
    "                'vegetarian': response.get('vegetarian', False),\n",
    "                'vegan': response.get('vegan', False),\n",
    "                'preparationMinutes': response.get('preparationMinutes'),\n",
    "                'cookingMinutes': response.get('cookingMinutes'),\n",
    "                'glutenFree': response.get('glutenFree', False),\n",
    "                'veryPopular': response.get('veryPopular', False),\n",
    "                'aggregateLikes': response.get('aggregateLikes'),\n",
    "                'instructions': response.get('instructions'),\n",
    "                'fetchDateTime': datetime.now(timezone.utc)\n",
    "            }\n",
    "            \n",
    "            # Save recipe (updated with new fields)\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO Recipes (\n",
    "                    id, image, title, readyInMinutes, servings, sourceUrl, sourceName,\n",
    "                    vegetarian, vegan, preparationMinutes, cookingMinutes,\n",
    "                    glutenFree, veryPopular, aggregateLikes, instructions, fetchDateTime\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", *recipe_data.values())\n",
    "            \n",
    "            # Save ingredients (unchanged)\n",
    "            for ingredient in response.get('extendedIngredients', []):\n",
    "                ingredient_data = {\n",
    "                    'recipeId': recipe_data['id'],\n",
    "                    'ingredientId': ingredient.get('id'),\n",
    "                    'name': ingredient.get('name'),\n",
    "                    'nameClean': ingredient.get('nameClean'),\n",
    "                    'original': ingredient.get('original'),\n",
    "                    'originalName': ingredient.get('originalName'),\n",
    "                    'amount': ingredient.get('amount'),\n",
    "                    'unit': ingredient.get('unit')\n",
    "                }\n",
    "                \n",
    "                self.cursor.execute(\"\"\"\n",
    "                    INSERT INTO RecipeIngredients (\n",
    "                        recipeId, ingredientId, name, nameClean, \n",
    "                        original, originalName, amount, unit\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", *ingredient_data.values())\n",
    "            \n",
    "            # Save cuisines (array handling)\n",
    "            for cuisine in response.get('cuisines', []):\n",
    "                # Ensure cuisine exists in lookup table\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    MERGE INTO Cuisines WITH (HOLDLOCK) AS target\n",
    "                    USING (SELECT ? AS name) AS source\n",
    "                    ON target.name = source.name\n",
    "                    WHEN NOT MATCHED THEN INSERT (name) VALUES (source.name);\n",
    "                \"\"\", cuisine)\n",
    "                \n",
    "                # Get cuisine ID\n",
    "                self.cursor.execute(\"SELECT id FROM Cuisines WHERE name = ?\", cuisine)\n",
    "                cuisine_id = self.cursor.fetchone()[0]\n",
    "                \n",
    "                # Link recipe to cuisine\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    INSERT INTO RecipeCuisines (recipeId, cuisineId)\n",
    "                    VALUES (?, ?)\n",
    "                \"\"\", recipe_data['id'], cuisine_id)\n",
    "            \n",
    "            # Save dish types (array handling)\n",
    "            for dish_type in response.get('dishTypes', []):\n",
    "                # Ensure dish type exists in lookup table\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    MERGE INTO DishTypes WITH (HOLDLOCK) AS target\n",
    "                    USING (SELECT ? AS name) AS source\n",
    "                    ON target.name = source.name\n",
    "                    WHEN NOT MATCHED THEN INSERT (name) VALUES (source.name);\n",
    "                \"\"\", dish_type)\n",
    "                \n",
    "                # Get dish type ID\n",
    "                self.cursor.execute(\"SELECT id FROM DishTypes WHERE name = ?\", dish_type)\n",
    "                dish_type_id = self.cursor.fetchone()[0]\n",
    "                \n",
    "                # Link recipe to dish type\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    INSERT INTO RecipeDishTypes (recipeId, dishTypeId)\n",
    "                    VALUES (?, ?)\n",
    "                \"\"\", recipe_data['id'], dish_type_id)\n",
    "            \n",
    "            self.conn.commit()\n",
    "            return True\n",
    "            \n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Database error with recipe {recipe_id}: {str(e)}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with recipe {recipe_id}: {str(e)}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "            \n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error parsing/saving recipe {recipe_id}: {str(e)}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with recipe {recipe_id}: {str(e)}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "    \n",
    "    def process_recipe(self, recipe_id):\n",
    "        # Fetch recipe data\n",
    "        recipe_id, response = self.fetch_recipe(recipe_id)\n",
    "        if not response:\n",
    "            return False\n",
    "            \n",
    "        # Save raw response\n",
    "        if not self.save_raw_response(recipe_id, response):\n",
    "            return False\n",
    "            \n",
    "        # Parse and save recipe data\n",
    "        if not self.parse_and_save_recipe(recipe_id, response):\n",
    "            return False\n",
    "            \n",
    "        # Download image if available\n",
    "        image_url = response.get('image')\n",
    "        if image_url:\n",
    "            success, file_ext = self.download_image(recipe_id, image_url)\n",
    "            if success:\n",
    "                print(f\"Successfully downloaded image for recipe {recipe_id} as {recipe_id}.{file_ext}\")\n",
    "            else:\n",
    "                print(f\"Failed to download image for recipe {recipe_id}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_last_recipe_id(self):\n",
    "        \"\"\"Get the highest recipe ID currently in the database\"\"\"\n",
    "        self.cursor.execute(\"SELECT MAX(recipeId) FROM RawRecipeData\")\n",
    "        result = self.cursor.fetchone()\n",
    "        return result[0] if result[0] is not None else 0\n",
    "\n",
    "    \n",
    "    def crawl_recipes(self, start_id=None, number_to_crawl=1500, batch_size=100, force_retry_failed=False):\n",
    "        \"\"\"\n",
    "        Crawl recipes with smart defaults:\n",
    "        - Defaults to 1500 recipes if number_to_crawl not specified\n",
    "        - Starts from last recipe ID + 1 if start_id not specified\n",
    "        \n",
    "        Args:\n",
    "            start_id (int): First recipe ID to crawl (None to auto-detect)\n",
    "            number_to_crawl (int): Total recipes to crawl (default 1500)\n",
    "            batch_size (int): Number of recipes per batch (default 100)\n",
    "            force_retry_failed (bool): Retry failed recipes (default False)\n",
    "        \"\"\"\n",
    "        # Determine starting ID\n",
    "        if start_id is None:\n",
    "            start_id = self.get_last_recipe_id() + 1\n",
    "            print(f\"Auto-starting from recipe ID {start_id}\")\n",
    "\n",
    "        current_id = start_id\n",
    "        processed_count = 0\n",
    "        last_successful_id = start_id - 1\n",
    "        \n",
    "        if force_retry_failed:\n",
    "            self.processed_ids = set()\n",
    "        else:\n",
    "            self.processed_ids = self._load_processed_ids()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            while processed_count < number_to_crawl:\n",
    "                batch = []\n",
    "                # Prepare next batch\n",
    "                while len(batch) < batch_size and (processed_count + len(batch)) < number_to_crawl:\n",
    "                    if current_id not in self.processed_ids:\n",
    "                        batch.append(current_id)\n",
    "                    current_id += 1\n",
    "                \n",
    "                if not batch:\n",
    "                    print(f\"No more recipes found. Reached ID {last_successful_id}\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"Processing IDs {batch[0]} to {batch[-1]} ({processed_count}/{number_to_crawl} processed)\")\n",
    "                \n",
    "                # Process batch\n",
    "                futures = {executor.submit(self.process_recipe, rid): rid for rid in batch}\n",
    "                for future in as_completed(futures):\n",
    "                    rid = futures[future]\n",
    "                    try:\n",
    "                        success = future.result()\n",
    "                        if success:\n",
    "                            processed_count += 1\n",
    "                            last_successful_id = rid\n",
    "                            self.processed_ids.add(rid)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing recipe {rid}: {str(e)}\")\n",
    "                \n",
    "                time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        print(f\"Completed crawling {processed_count} recipes (target: {number_to_crawl})\")\n",
    "        print(f\"Last successful ID: {last_successful_id}\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"APIKEY\"\n",
    "DB_CONNECTION_STRING = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=(localdb)\\\\MSSQLLocalDB;DATABASE=RecipeDB;Trusted_Connection=yes;\"\n",
    "# IMAGE_STORAGE_PATH = \"./recipe_images\"  # Directory to store downloaded images\n",
    "IMAGE_STORAGE_PATH = \"C:\\\\Users\\\\paili\\\\recipe_images\"  # Directory to store downloaded images\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = SpoonacularCrawler(\n",
    "        api_key=API_KEY,\n",
    "        db_connection_string=DB_CONNECTION_STRING,\n",
    "        image_storage_path=IMAGE_STORAGE_PATH,\n",
    "        max_workers=1,\n",
    "        request_timeout=30\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Start crawling from Last crowled ID and crawled number_to_crawl records.\n",
    "        crawler.crawl_recipes(number_to_crawl=10)\n",
    "    finally:\n",
    "        crawler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26971f2f-c9e1-423e-845b-1650bc4bb0b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
